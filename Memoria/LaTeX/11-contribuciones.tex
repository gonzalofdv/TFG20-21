\noindent En este capitulo se explica la implicación de cada miembro del equipo y las tareas que han realizado para la elaboración de este trabajo. Cabe destacar que los tres miembros estaban en constante contacto y se ayudaban mutuamente, tanto en la fase de investigación como en la de desarrollo, experimentación y escritura de la memoria. Trabajar en equipo para realizar las tareas era indispensable y se ha llevado a cabo con éxito, ya que todos los miembros han adquirido conocimientos importantes y han aportado en todas las áreas del trabajo.


\section{Gonzalo Fernández Megía}
\noindent Al inicio, se realizó una búsqueda exhaustiva de información relacionada con el tema, en concreto con proyectos ya realizados por otros investigadores. Leímos gran cantidad de artículos, de los que se realizaban resúmenes, extrayendo conclusiones, plasmándolas en tablas. Se trató de un proceso que nos llevó gran cantidad de horas, puesto que la intención era tener una base sólida de conocimientos para poder después desarrollar nuestro trabajo propio, de manera que estuviera bien fundamentado y tuviera la calidad necesaria. Toda esta información ha sido utilizada a lo largo del proyecto para replicar otros trabajos. En mi caso, además hice hincapié en la investigación de los conjuntos de datos utilizados por estudios similares, así como en otros trabajos exclusivamente de \gls{ML}, información que nos sería útil para la construcción de nuestro \textit{dataset}, y que luego nos serviría para hacer la comparación con nuestro \textit{dataset}, superior en tamaño.

Una vez elegido el camino a seguir y los métodos a utilizar para la detección de ransomware, se realiza otra recogida de información, esta vez para contextualizar el trabajo y proporcionar conocimientos sobre las materias claves del estudio: el malware, el ransomware y los métodos más importantes para analizarlos. Al empezar a escribir la memoria del trabajo, realicé una búsqueda de información sobre la situación social y económica en relación a los ataques ransomware para enmarcar nuestro trabajo en la actualidad y poder preparar una introducción adecuada al momento. Es destacable también el aprendizaje de lenguaje Latex para el desarrollo y escritura de la memoria, haciendo uso de manuales, que nos proporpcionó un mecanismo muy potente a la hora de plasmar toda la información con apoyo visual como tablas, figuras o esquemas. Posteriormente se continúa estudiando para obtener material, en este caso, centrándonos más en los aspectos tecnológicos y científicos para poder aplicarlos en nuestro trabajo de manera independiente. Tras toda esta investigación, se decide realizar un modelo de \gls{ML} para la identificación de ransomware, puesto que es el mecanismo más usado por la mayoría de los investigadores y el que mejor resultados ofrece. Como características elegimos las llamadas a la \gls{API} de Windows, puesto que son relativamente fáciles de extraer y constituyen un artefacto sólido para la detección de ransomware debido a que trazan el comportamiento de este en el sistema víctima.

A continuación, debíamos comenzar a construir un \textit{dataset} para alimentar el modelo y comenzar a hacer pruebas. Tras la investigación relativa a los conjuntos de datos, pudimos solicitar a aquellos autores cuyos \textit{dataset} se consideraban de mayor calidad para realizar las primeras aproximaciones y establecer una referencia que posteriormente nos facilitaría la tarea. Resulté de ayuda para mis compañeros en el desarrollo de los \textit{scripts} necesarios para la extracción y limpieza de los datos para la creación de nuestro conjunto de datos propio, proceso fundamental en el desarrolo del proyecto, puesto que era de vital importancia obtener un conjunto de datos adecuado y lo suficientemente grande con la finalidad de alimentar al modelo de \gls{ML} con datos reales y consistentes, obtener óptimos resultados y evitar el \textit{overfitting}. Primero se construye el \textit{dataset binario}, en el que solo se cuentan las apariciones de las llamadas a la \gls{API} de Windows. Tras aplicar el algoritmo de limpieza de datos se ve reducido de manera brusca. Debido a este factor, y al de ampliar la investigación para obtener otro enfoque, se construye un segundo conjunto de datos, el \textit{dataset binario}. Los datos que contiene se forman sumando la cantidad de llamadas a la \gls{API} de Windows que realizan los procesos pertenecientes a las muestras obtenidas. Obtenemos así un \textit{dataset} de mayor tamaño tras la limpieza y unos mejores resultados posteriores.

De manera paralela, se empieza a desarrollar el modelo. Lo primero que hice fue aprender a construir un modelo desde cero mediante el uso de la librería de Python \textit{sci-kit Learn}, para lo cual, tuve que leer la documentación ofrecida por los desarrolladores en su manual de uso. En los inicios tuve que llevar a cabo gran cantidad de pruebas con distintos conjuntos de datos y diversos algoritmos para entender el funcionamiento del sistema y obtener la configuración y parámetros idóneos para nuestro proyecto. Una vez construido un modelo mínimamente funcional, comienzo a desarrollar los experimentos en mi máquina virtual Kali Linux 2020.3, y a registrar los primeros resultados. Decidimos llevar a cabo la extracción de gran variedad de métricas para analizar mejor dichos resultados y poder mejorar la efectividad del modelo. Asimismo, se utilizan un total de 7 algoritmos de \gls{ML} distintos, consiguiendo una gran variedad en los experimentos y unos resultados contrastados. Cuando el modelo alcanza la efectividad que esperábamos, se lleva a cabo una recogida de los resultados, los introducimos en tablas y realizo la labor de plasmarlos en gráficas para su mejor entendimiento.  

También contribuyo a la creación de un \href{https://gitlab.fdi.ucm.es/marina.lopez/tfg-ransomware-20-21}{repositorio online} para el almacenamiento de los ficheros relacionados con el \textit{dataset} y el modelo. Además, junto a mi compañero Gonzalo Figueroa, hemos desarrollado un manual para su mejor entendimiento y uso de manera más sencilla para todo tipo de usuario.

Es necesaria la mención al desarrollo de un diagrama de Gantt, que muestra las actividades realizadas por el grupo de trabajo a lo largo del tiempo. Este facilita el entendimiento de la pauta de trabajo que hemos seguido y cuáles son los ciclos de tiempo de realizar un estudio como el nuestro.

Finalmente, aunando resultados obtenidos e información recogida, somos capaces de redactar una serie de conclusiones finales para afianzar nuestro estudio, que traduzco al inglés junto con mi compañero Gonzalo Figueroa.

\newpage
\section{Gonzalo Figueroa del Val}
\noindent A la hora de iniciar el proyecto, lo primero fue realizar una amplia investigación acerca del tema a tratar para construir una base de conocimiento de la que partir. Para esta investigación, hemos leído y analizado gran cantidad de trabajos y libros sobre análisis y detección de malware, haciendo hincapié en el detección de ransomware, aprendiendo sobre diferentes técnicas y enfoques para ello. La documentación leída la hemos obtenido de fuentes verificadas, como la biblioteca online de la Universidad Complutense de Madrid o editoriales como \textit{SpringerLink} o \textit{IEEE Xplore}, entre otras, realizando las búsquedas a través de \textit{Google Scholar}. Además, recogimos las referencias correspondientes en formato \textit{BibTeX} para incorporarlas a nuestra memoria.

El siguiente paso fue montar un laboratorio de análisis de malware para extraer características de las muestras que teníamos para analizar. El laboratorio lo construí en mi equipo con sistema operativo Linux con distribución Ubuntu LTS 18.0.5 e incorporando la herramienta Cuckoo Sandbox, la cual ha sido esencial para el desarrollo del trabajo. El proceso de montar el laboratorio fue bastante tedioso puesto que el trabajo con este entorno era nuevo para mí y aparecían continuamente errores. Finalmente, consultando el manual de Cuckoo Sandbox y otras fuentes externas, el laboratorio de análisis de malware quedó construido y listo para trabajar, desarrollando mi propio manual de montaje y configuración que se puede contemplar en la Sección \ref{sec:lab} del Capítulo \ref{Capitulo5}.

Mientras no dejábamos de empaparnos de información, se llegó a la conclusión de que la característica principal que alimentaría nuestro trabajo y por lo tanto también el modelo de \gls{ML} que desarrollaríamos serían las llamadas a la \gls{API} de Windows. Esta característica se puede obtener fácilmente de los informes generados por Cuckoo Sandbox al analizar muestras ya que se concentran en una sección concreta llamada \textit{'apistats'}. Con esto, empezamos a trabajar en la extracción de las llamadas a la \gls{API} de Windows por cada proceso de ejecución del malware dando lugar a la creación de un \textit{script} Python para procesar la información que acabaría construyendo nuestro \textit{dataset}. 

Con nuestros informes y los proporcionados por el grupo de investigación dediqué bastante tiempo para, con la ayuda de mis compañeros, escribir los \textit{scripts} definitivos que procesarían correctamente todos los informes y limpiarían los datos adecuadamente para así construir los dos \textit{datasets} finales que se utilizarían para los experimentos. El primero de ellos, nombrado como \textit{dataset binario} representa de manera binaria la llamada o no de cada \gls{API} durante la actuación de cada muestra y se construyó basándonos en otros trabajos que utilizaban también esta representación binaria. La creación del segundo \textit{dataset} nombrado como \textit{dataset suma} cuenta para cada llamada \gls{API} el número de veces que ha sido realizada por la muestra, diferenciándonos así de otros trabajos utilizando este \textit{dataset}. Para realizar todo este proceso se tuvo en cuenta las buenas prácticas para construir un \textit{dataset} adecuado, teniendo en cuenta las fases de extracción, transformación, limpieza y carga.

A la hora de trabajar con \gls{ML}, se han tenido en cuenta todas las fases correspondientes al proceso de desarrollar un modelo de aprendizaje automático, desde la definición del problema hasta la prueba del modelo. Probamos multitud de algoritmos y configuraciones para los dos \textit{datasets}, probando diferentes técnicas de escalado, repartos de datos de entrenamiento y prueba o validaciones cruzadas con varios valores. Una vez recopilados los resultados ofrecidos por el modelo, los plasmamos en la memoria y realizamos una comparación entre los algoritmos utilizados y las distintas configuraciones probadas. Además, comparamos los resultados con los trabajos del estado del arte más relevantes. Por último, se redactaron las conclusiones finales para terminar la memoria del trabajo.

Cabe mencionar que también hemos creado un \href{https://gitlab.fdi.ucm.es/marina.lopez/tfg-ransomware-20-21}{repositorio online} donde se almacenan los datos y ficheros relacionados con la construcción del \textit{dataset} y el modelo y donde junto a mi compañero Gonzalo Fernández se ha escrito un manual de uso del sistema propuesto, incluyendo la instalación de requisitos previa al desarrollo del \textit{dataset} y del modelo, la construcción del laboratorio y del \textit{dataset} y la ejecución del modelo de \gls{ML} desarrollado con sus diferentes configuraciones.

Durante todo el proceso de investigación y experimentación he realizado consultas a los manuales de las diferentes herramientas o módulos de Python de los que he hecho uso, así como realizado consultas concretas en foros muy reputados como \textit{StackOverflow}. En cuanto al equipo hemos trabajado con una comunicación constante estructurando tareas y colaborando entre todos contribuyendo al proyecto de forma equilibrada y apoyándonos en las dificultades encontradas.


\section{Marina López Osorio}
\noindent Al comenzar este trabajo, el primer paso fue buscar información, artículos y todo tipo de estudios sobre el malware, su análisis y detección para luego centrarse en el ransomware. Para encontrar toda esta información, mis compañeros y yo usamos la herramienta llamada \textit{Google Scholar} que permitía buscar con facilidad artículos científicos y leer los que eran más citados por otros estudios, lo que indicaba que era un artículo útil y con información veraz. Leyendo estos artículos aprendí los conceptos fundamentales del malware y ransomware, como se distribuían e infectaban a los sistemas, cómo se podían analizar y detectar, los diferentes tipos y familias, etc. También buscamos información sobre el aprendizaje automático, ya que muchos trabajos lo utilizaban para la detección de amenazas, y una asignatura que me ayudó a comprender los conceptos y me enseñó algunos algoritmos de aprendizaje automático fue Ingeniería del Conocimiento. Además de eso, realicé algunos cursos de Coursera sobre aprendizaje automático para afianzar lo aprendido. Con todos estos conocimientos ya nos pudimos centrar en cómo implementar un modelo de \gls{ML} para detectar ransomware.  

El método más común que utilizan los trabajos para detectar amenazas tanto de ransomware como de malware es la extracción de las llamadas a la \gls{API} de Windows que realizan al infectar el sistema de la víctima. Estas llamadas son las que transforman en vectores de características para alimentar al modelo de \gls{ML} para detectar ransomware. Para la extracción de características, la mayoría de trabajos utilizaban la herramienta llamada Cuckoo Sandbox, por lo que buscamos información sobre ella y empecé a hacer pruebas con muestras en la versión online de la herramienta. Cuckoo Sandbox devolvía unos reportes en formato json, por lo que tuve que escribir un \textit{script} en Python que extrajese las llamadas a la \gls{API}. Había programado antes en Python pero muy pocas veces en comparación con otros lenguajes como C++ o Java, por lo que tuve que refrescarme la memoria y familiarizarme con la librería de \textit{scikit-learn} para implementar los algoritmos de aprendizaje automático, y otras librerías como \textit{Pandas} para la lectura y análisis de las muestras. Este \textit{script} que extraía las \gls{API}s de los reportes de Cuckoo Sandbox al final fue reemplazado por otro, ya que los datos de las \gls{API}s los terminamos cogiendo directamente de otra fuente con los reportes ya analizados por Cuckoo, pero sirvió para familiarizarme con la herramienta y saber qué se sacaba exactamente de las muestras.

Al tener el vector de las características, era hora de construir el modelo. Busqué tutoriales de todo tipo y empecé a escribir el código, utilizando simplemente un algoritmo de aprendizaje supervisado, que era \gls{SVM}. Después de muchas pruebas con diferentes datos, tanto de nuestras muestras como las de otros trabajos, junté mi \textit{script} con el de mis compañeros y así pudimos comparar posibles errores y tener más algoritmos para obtener resultados con todos ellos y ver cual era el que mejor rendía. Para obtener estos resultados, se utilizaron las métricas del módulo \textit{sklearn.metrics} de la librería \textit{scikit-learn}. Busqué información de todas las métricas que se podían sacar y elegí las que me parecían más relevantes y las que usaban otros trabajos relacionados. Para la validación de los resultados, decidimos usar la validación cruzada K-Fold, por lo que busqué información de cómo implementarla y lo añadimos al código.

El modelo sufrió varios cambios debido a que nuestro \textit{dataset} también los tuvo, y después obtener los dos \textit{datasets} finales (\textit{dataset binario} y \textit{dataset suma}), se pudo experimentar más a fondo con los algoritmos y obtener los resultados definitivos. Al ver que el \textit{dataset suma} tenía valores no binarios, era necesario escalar los datos, ya que el tiempo de ejecución era demasiado alto y algunos algoritmos no funcionaban correctamente. Investigué y encontré dos escaladores de la librería \textit{scikit-learn} que podríamos utilizar, el \textit{MinMaxScaler} y el \textit{StandardScaler}. Experimenté con los dos escaladores sobre el \textit{dataset suma} y el \textit{StandardScaler} obtuvo mejores resultados, y esto se confirmó cuando mi compañero realizó las gráficas que lo mostraban de manera clara. Posteriormente pasé a experimentar con el \textit{dataset binario} y a obtener los resultados. De todos los algoritmos usados, vi que el \gls{RF} era el que mejores resultados obtenía, por lo que ese fue el que se usó para construir el modelo final de este trabajo.

Durante las etapas de investigación y desarrollo del sistema propuesto, se fue realizando la memoria en \textit{LaTeX}, una herramienta para redactar artículos y documentos en el ámbito de la investigación. Nunca la había usado antes, pero gracias a mis tutores y al manual que se puede encontrar en la web, no fue difícil adaptarme a la herramienta y me pareció muy útil, debido a que facilito enormemente la realización de las citas. Escribí todos los conocimientos que iba adquiriendo y el desarrollo del trabajo a medida que avanzábamos, coordinándome con mis compañeros para no repetir información. También me encargué de la traducción al inglés de varias partes de la introducción. 

Contribuí a la creación del \href{https://gitlab.fdi.ucm.es/marina.lopez/tfg-ransomware-20-21}{repositorio online} y público en Gitlab para el almacenamiento de todos los ficheros relacionados con el desarrollo del modelo.

A lo largo del desarrollo de este trabajo he adquirido conocimientos tanto técnicos como de trabajo en equipo, ya que tenia que estar en constante contacto con mis compañeros y asistir a reuniones semanales con mis tutores, siendo una experiencia importante para el futuro en una empresa. También adquirí la habilidad de extraer información de fuentes científicas, distinguiendo cuales son las fiables de las que no. Este trabajo ha sido una buena experiencia que me ha permitido aprender mucho sobre la ciberseguridad y el aprendizaje automático, dos áreas que actualmente están muy presentes en el mundo laboral. 



